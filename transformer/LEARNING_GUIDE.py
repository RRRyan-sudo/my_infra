"""
Transformer å®Œæ•´å­¦ä¹ æŒ‡å—

æœ¬è„šæœ¬æä¾›äº†Transformeræ¶æ„çš„å®Œæ•´å®ç°å’Œå­¦ä¹ è·¯å¾„ã€‚

å¿«é€Ÿå¼€å§‹æ­¥éª¤ï¼š
1. å®‰è£…ä¾èµ–: pip install -r requirements.txt
2. è¿è¡Œå„ä¸ªæ¨¡å—è¿›è¡Œå­¦ä¹ :
   - python src/01_positional_encoding.py      # å­¦ä¹ ä½ç½®ç¼–ç 
   - python src/02_attention.py                # å­¦ä¹ æ³¨æ„åŠ›æœºåˆ¶
   - python src/04_feed_forward.py             # å­¦ä¹ å‰é¦ˆç½‘ç»œ
   - python src/05_encoder_layer.py            # å­¦ä¹ ç¼–ç å™¨å±‚
   - python src/06_decoder_layer.py            # å­¦ä¹ è§£ç å™¨å±‚
   - python src/07_transformer.py              # å­¦ä¹ å®Œæ•´æ¨¡å‹
3. æ‰“å¼€Jupyter notebooksè¿›è¡Œäº¤äº’å¼å­¦ä¹ 

å­¦ä¹ è·¯å¾„æŒ‡å¯¼ï¼š
"""

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)


def print_welcome():
    """æ‰“å°æ¬¢è¿ä¿¡æ¯"""
    print("\n" + "=" * 70)
    print(" " * 15 + "æ¬¢è¿æ¥åˆ° Transformer æ¶æ„å­¦ä¹ ä¹‹æ—…ï¼")
    print("=" * 70)
    
    learning_path = """
ğŸ“š å­¦ä¹ è·¯å¾„ï¼ˆå»ºè®®é¡ºåºï¼‰ï¼š

ã€ç¬¬ä¸€é˜¶æ®µã€‘åŸºç¡€æ¦‚å¿µ
  1ï¸âƒ£  ä½ç½®ç¼–ç  (Positional Encoding)
      ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ
      - Transformeræ˜¯å¹¶è¡Œå¤„ç†åºåˆ—çš„ï¼Œæ²¡æœ‰é¡ºåºä¿¡æ¯
      - ä½ç½®ç¼–ç å°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ°å‘é‡ä¸­
      å…³é”®å…¬å¼: PE(pos, 2i) = sin(pos/10000^(2i/d))
                PE(pos, 2i+1) = cos(pos/10000^(2i/d))

  2ï¸âƒ£  æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism)
      ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›ï¼Ÿ
      - åœ¨å¤„ç†æ¯ä¸ªä½ç½®æ—¶ï¼Œå…³æ³¨ç›¸å…³çš„å…¶ä»–ä½ç½®
      - Query (æŸ¥è¯¢): "æˆ‘æƒ³çŸ¥é“ä»€ä¹ˆ"
      - Key (é”®): "æ¯ä¸ªä½ç½®æ˜¯ä»€ä¹ˆ"
      - Value (å€¼): "æ¯ä¸ªä½ç½®çš„ä¿¡æ¯"
      å…³é”®å…¬å¼: Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V

ã€ç¬¬äºŒé˜¶æ®µã€‘æ ¸å¿ƒç»„ä»¶
  3ï¸âƒ£  å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)
      ä¸ºä»€ä¹ˆä½¿ç”¨å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Ÿ
      - ä¸åŒçš„å¤´å¯ä»¥å­¦ä¹ ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´
      - å¹¶è¡Œè¿è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œç„¶åè¿æ¥ç»“æœ
      - å¢å¼ºäº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›

  4ï¸âƒ£  å‰é¦ˆç½‘ç»œ (Feed Forward Network)
      ç»“æ„: Linear(d_model â†’ d_ff) â†’ ReLU â†’ Linear(d_ff â†’ d_model)
      ç‰¹ç‚¹: åœ¨æ¯ä¸ªä½ç½®ç‹¬ç«‹åº”ç”¨ç›¸åŒçš„å‰é¦ˆç½‘ç»œ
           é€šå¸¸ d_ff = 4 Ã— d_model

  5ï¸âƒ£  å±‚æ„ä»¶ï¼ˆResidual & Layer Normï¼‰
      æ®‹å·®è¿æ¥: x = x + sublayer(x)
        - å…è®¸æ¢¯åº¦ç›´æ¥æµåŠ¨
        - ç¼“è§£æ·±ç½‘ç»œçš„è®­ç»ƒå›°éš¾
      
      å±‚å½’ä¸€åŒ–: åœ¨ç‰¹å¾ç»´åº¦ä¸Šè¿›è¡Œå½’ä¸€åŒ–
        - ç¨³å®šè®­ç»ƒ
        - ç‹¬ç«‹äºæ‰¹æ¬¡å¤§å°

ã€ç¬¬ä¸‰é˜¶æ®µã€‘æ¨¡å‹æ¶æ„
  6ï¸âƒ£  ç¼–ç å™¨å±‚ (Encoder Layer)
      ç»“æ„:
        x' = MultiHeadAttention(x, x, x)
        x = LayerNorm(x + Dropout(x'))
        x' = FeedForward(x)
        x = LayerNorm(x + Dropout(x'))

  7ï¸âƒ£  è§£ç å™¨å±‚ (Decoder Layer)
      ç»“æ„:
        x' = MultiHeadAttention(x, x, x, causal_mask)  # è‡ªæ³¨æ„åŠ›
        x = LayerNorm(x + Dropout(x'))
        x' = MultiHeadAttention(x, encoder_out, encoder_out)  # äº¤å‰æ³¨æ„åŠ›
        x = LayerNorm(x + Dropout(x'))
        x' = FeedForward(x)
        x = LayerNorm(x + Dropout(x'))

  8ï¸âƒ£  å®Œæ•´Transformeræ¨¡å‹
      åŒ…å«: Embedding + PositionalEncoding + Encoder + Decoder
      æ•°æ®æµ: æºåºåˆ— â†’ ç¼–ç å™¨ â†’ è§£ç å™¨ â†’ ç›®æ ‡åºåˆ—é¢„æµ‹

ã€ç¬¬å››é˜¶æ®µã€‘å®è·µåº”ç”¨
  9ï¸âƒ£  æ•°æ®é¢„å¤„ç†
      - åˆ†è¯ï¼ˆTokenizationï¼‰
      - æ„å»ºè¯æ±‡è¡¨ï¼ˆVocabularyï¼‰
      - å¡«å……å’Œåºåˆ—é•¿åº¦å¤„ç†

  ğŸ”Ÿ æ¨¡å‹è®­ç»ƒ
      - å®šä¹‰æŸå¤±å‡½æ•°ï¼ˆé€šå¸¸ä½¿ç”¨äº¤å‰ç†µï¼‰
      - é€‰æ‹©ä¼˜åŒ–å™¨ï¼ˆAdamï¼‰
      - å®ç°è®­ç»ƒå¾ªç¯
      - è¯„ä¼°æ¨¡å‹æ€§èƒ½

ğŸ“Š å…³é”®æ•°å­¦å…¬å¼é€ŸæŸ¥ï¼š

1. æ³¨æ„åŠ›: A(Q,K,V) = softmax(QK^T/âˆšd_k)V

2. å¤šå¤´æ³¨æ„åŠ›: MultiHead(Q,K,V) = Concat(headâ‚,...,head_h)W^O
              å…¶ä¸­ headáµ¢ = Attention(QWáµ¢^Q, KWáµ¢^K, VWáµ¢^V)

3. å‰é¦ˆ: FFN(x) = max(0, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚

4. ä½ç½®ç¼–ç : PE(pos,2i) = sin(pos/10000^(2i/d))
           PE(pos,2i+1) = cos(pos/10000^(2i/d))

5. æ®‹å·®+å½’ä¸€åŒ–: y = LayerNorm(x + f(x))

ğŸ’¡ å­¦ä¹ å»ºè®®ï¼š
  - å…ˆç†è§£æ•°å­¦æ¦‚å¿µï¼Œå†çœ‹ä»£ç å®ç°
  - è¿è¡Œæ¯ä¸ªæ¨¡å—çš„æµ‹è¯•ä»£ç ï¼Œè§‚å¯Ÿè¾“å…¥è¾“å‡ºå½¢çŠ¶
  - ä½¿ç”¨print()å’Œå¯è§†åŒ–å·¥å…·ç†è§£æ•°æ®æµ
  - ä¿®æ”¹å‚æ•°ï¼Œè§‚å¯Ÿå¯¹æ¨¡å‹çš„å½±å“
  - å®ç°ä¸€ä¸ªç®€å•çš„æœºå™¨ç¿»è¯‘ä»»åŠ¡æ¥å·©å›ºå­¦ä¹ 

ğŸš€ å¿«é€Ÿå¼€å§‹ï¼š
  
  # 1. å®‰è£…ä¾èµ–
  pip install -r requirements.txt
  
  # 2. é€ä¸ªå­¦ä¹ å„ä¸ªæ¨¡å—
  python src/01_positional_encoding.py
  python src/02_attention.py
  python src/04_feed_forward.py
  python src/05_encoder_layer.py
  python src/06_decoder_layer.py
  python src/07_transformer.py
  
  # 3. è¿›è¡Œäº¤äº’å¼å­¦ä¹ 
  jupyter notebook notebooks/
  
  # 4. è¿è¡Œå®è·µç¤ºä¾‹
  python examples/train_example.py

ğŸ“– å‚è€ƒèµ„æºï¼š
  - è®ºæ–‡: "Attention Is All You Need" (Vaswani et al., 2017)
  - ç½‘å€: https://arxiv.org/abs/1706.03762
  - ä»£ç : https://github.com/pytorch/examples/blob/master/word_language_model/model.py

ğŸ¯ é¢„æœŸå­¦ä¹ æˆæœï¼š
  âœ“ æ·±å…¥ç†è§£ Transformer çš„æ¯ä¸ªç»„ä»¶
  âœ“ èƒ½å¤Ÿä»é›¶å®ç°å®Œæ•´çš„ Transformer æ¨¡å‹
  âœ“ ç†è§£ä½ç½®ç¼–ç ã€æ³¨æ„åŠ›æœºåˆ¶çš„æ•°å­¦åŸç†
  âœ“ äº†è§£å¦‚ä½•å°† Transformer åº”ç”¨äºå®é™…ä»»åŠ¡
  âœ“ èƒ½å¤Ÿè°ƒè¯•å’Œä¼˜åŒ– Transformer æ¨¡å‹

æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·å‚è€ƒå¯¹åº”æ¨¡å—ä¸­çš„è¯¦ç»†æ³¨é‡Šï¼
    """
    
    print(learning_path)
    print("=" * 70 + "\n")


def verify_installation():
    """éªŒè¯æ‰€éœ€çš„åº“æ˜¯å¦å·²å®‰è£…"""
    print("æ£€æŸ¥ä¾èµ–åº“...")
    
    try:
        import torch
        print(f"âœ“ PyTorch {torch.__version__}")
    except ImportError:
        print("âœ— PyTorch æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install torch")
        return False
    
    try:
        import numpy
        print(f"âœ“ NumPy {numpy.__version__}")
    except ImportError:
        print("âœ— NumPy æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install numpy")
        return False
    
    print("\næ‰€æœ‰ä¾èµ–éƒ½å·²å®‰è£…ï¼\n")
    return True


def main():
    """ä¸»å‡½æ•°"""
    print_welcome()
    
    if not verify_installation():
        print("è¯·å…ˆå®‰è£…æ‰€éœ€çš„åº“")
        return
    
    print("ğŸ’¡ ç°åœ¨ä½ å¯ä»¥å¼€å§‹å­¦ä¹  Transformer äº†ï¼\n")
    print("å»ºè®®çš„å­¦ä¹ æ­¥éª¤ï¼š\n")
    
    steps = [
        ("1", "src/01_positional_encoding.py", "å­¦ä¹ ä½ç½®ç¼–ç "),
        ("2", "src/02_attention.py", "å­¦ä¹ æ³¨æ„åŠ›æœºåˆ¶å’Œå¤šå¤´æ³¨æ„åŠ›"),
        ("3", "src/04_feed_forward.py", "å­¦ä¹ å‰é¦ˆç½‘ç»œ"),
        ("4", "src/05_encoder_layer.py", "å­¦ä¹ ç¼–ç å™¨å±‚"),
        ("5", "src/06_decoder_layer.py", "å­¦ä¹ è§£ç å™¨å±‚"),
        ("6", "src/07_transformer.py", "å­¦ä¹ å®Œæ•´Transformeræ¨¡å‹"),
    ]
    
    for num, file_path, description in steps:
        print(f"  æ­¥éª¤ {num}: python {file_path}")
        print(f"           {description}\n")
    
    print("\nå¼€å§‹å­¦ä¹ å§ï¼ç¥ä½ å­¦ä¹ æ„‰å¿«ï¼ğŸš€\n")


if __name__ == "__main__":
    main()
