"""
强化学习速查手册
Quick Reference for Reinforcement Learning
"""

QUICK_REFERENCE = """
╔══════════════════════════════════════════════════════════════════════════════╗
║                                                                              ║
║                   强化学习快速参考 - Cheat Sheet                             ║
║                                                                              ║
║                Quick Reference Guide for RL Learning                         ║
║                                                                              ║
╚══════════════════════════════════════════════════════════════════════════════╝


【核心符号速查表】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

基本符号:
  s, s'     状态 (state)
  a, a'     动作 (action)
  r, R      奖励 (reward)
  γ         折扣因子 (discount factor, 0-1)
  α         学习率 (learning rate, 0-1)
  ε         探索率 (exploration rate, 0-1)
  π         策略 (policy)
  π(a|s)    策略：状态s下采取动作a的概率
  
价值函数:
  V(s)      状态价值：从状态s出发的期望累积奖励
  Q(s,a)    动作价值：状态s采取动作a的期望累积奖励
  A(s,a)    优势函数：Q(s,a) - V(s)
  
回报:
  G_t       累积回报 = r_t + γ*r_{t+1} + γ²*r_{t+2} + ...
  G_t(λ)    λ-回报 = (1-λ)*Σ λⁿ*G_{t:t+n+1}
  
梯度:
  ∇         梯度操作符
  ∇_θ J(θ)  策略目标函数的梯度


【贝尔曼方程】 - 最重要！
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

递推关系:
┌─────────────────────────────────────────┐
│ V(s)  = E[r + γV(s')]                   │  状态价值贝尔曼方程
│ Q(s,a) = E[r + γQ(s',a')]               │  动作价值贝尔曼方程
└─────────────────────────────────────────┘

展开形式 (已知模型P和R):
┌─────────────────────────────────────────┐
│ V(s) = Σ_a π(a|s) Σ_{s',r} P(s',r|s,a) │
│        [r + γV(s')]                     │
│                                         │
│ Q(s,a) = Σ_{s',r} P(s',r|s,a)          │
│          [r + γΣ_a' π(a'|s')Q(s',a')]  │
└─────────────────────────────────────────┘

最优性贝尔曼方程:
┌─────────────────────────────────────────┐
│ V*(s) = max_a E[r + γV*(s')]            │
│ Q*(s,a) = E[r + γmax_a' Q*(s',a')]     │
└─────────────────────────────────────────┘

最优策略提取:
┌─────────────────────────────────────────┐
│ π*(s) = argmax_a Q*(s,a)                │
└─────────────────────────────────────────┘


【5大算法速查】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

1️⃣ 策略迭代 (Policy Iteration)
┌─────────────────────────────────────────┐
│ 步骤:                                    │
│  1. 评估: V^π(s) = E[r + γV^π(s')]      │
│  2. 改进: π'(s) = argmax Q^π(s,a)       │
│  3. 重复直到π收敛                       │
│                                         │
│ 特点: 二层循环，收敛快，计算复杂度高    │
│ 时间复杂度: O(S²A) per iteration        │
│ 需要: 完整环境模型                       │
└─────────────────────────────────────────┘

2️⃣ 价值迭代 (Value Iteration)
┌─────────────────────────────────────────┐
│ 更新: V(s) ← max_a E[r + γV(s')]        │
│ 收敛: ||V - V_old|| < ε                 │
│                                         │
│ 特点: 单层循环，快速，直接求最优值     │
│ 时间复杂度: O(SA) per iteration         │
│ 需要: 完整环境模型                       │
└─────────────────────────────────────────┘

3️⃣ 蒙特卡洛 (Monte Carlo)
┌─────────────────────────────────────────┐
│ 回报: G = r_1 + γr_2 + γ²r_3 + ...      │
│ 更新: V(s) ← V(s) + α(G - V(s))         │
│       (First-visit只在首次访问时更新)   │
│                                         │
│ 特点: 采样完整episode，高方差，无偏    │
│ 收敛: O(1/n) 其中n是访问次数            │
│ 需要: 仅需交互能力                       │
└─────────────────────────────────────────┘

4️⃣ Q-Learning (TD离策略)
┌─────────────────────────────────────────┐
│ 更新: Q(s,a) ← Q(s,a) + α[r + γmax Q    │
│               (s',a') - Q(s,a)]         │
│               (使用贪心目标策略)         │
│                                         │
│ 特点: 单步更新，快速，学习最优策略    │
│ 自举: 用自己的估计更新自己              │
│ 需要: 仅需交互能力                       │
│ GLIE: ε → 0, ε衰减满足条件              │
└─────────────────────────────────────────┘

5️⃣ 策略梯度 (Policy Gradient)
┌─────────────────────────────────────────┐
│ 梯度: ∇J(θ) = E[∇log π_θ(a|s)Q(s,a)]  │
│ 更新: θ ← θ + α∇J(θ)                   │
│                                         │
│ REINFORCE:                              │
│   θ ← θ + α∇log π_θ(a|s) G             │
│   (使用实际回报G)                       │
│                                         │
│ Actor-Critic:                           │
│   θ_actor ← θ + α∇log π Q_critic       │
│   φ_critic ← φ + β(r + γV_φ - V_φ)²   │
│                                         │
│ 特点: 参数化策略，支持连续动作          │
│ 需要: 仅需交互能力                       │
└─────────────────────────────────────────┘


【算法对比速查表】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

┌──────────────┬────────┬───────┬──────┬──────┬────────┬─────────┐
│   算法       │ 模型   │ 更新  │ 偏差 │ 方差 │ 收敛   │ 状态空间│
├──────────────┼────────┼───────┼──────┼──────┼────────┼─────────┤
│ PI策略迭代   │ 需要✓  │多步  │ 无   │ 低   │ 快     │ 离散    │
│ VI价值迭代   │ 需要✓  │单步  │ 无   │ 低   │ 快     │ 离散    │
│ MC蒙特卡洛   │ 不需✗  │episode│ 无   │ 高   │ 中等   │ 离散    │
│ Q-Learning   │ 不需✗  │单步  │ 有   │ 中   │ 快     │ 离散    │
│ Sarsa        │ 不需✗  │单步  │ 无   │ 低   │ 较慢   │ 离散    │
│ Expected Sarsa│不需✗ │单步  │ 无   │ 低   │ 中等   │ 离散    │
│ REINFORCE    │ 不需✗  │episode│ 无   │ 高   │ 较慢   │ 任意    │
│ Actor-Critic │ 不需✗  │单步  │ 有   │ 低   │ 中等   │ 任意    │
└──────────────┴────────┴───────┴──────┴──────┴────────┴─────────┘


【关键代码模式】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

ε-贪心策略:
┌─────────────────────────────────────┐
│ if random() < ε:                    │
│     action = random_action()        │
│ else:                               │
│     action = argmax(Q[state])       │
└─────────────────────────────────────┘

Softmax策略:
┌─────────────────────────────────────┐
│ π(a|s) = exp(Q(s,a)/τ) / Σexp(...)  │
│ action = sample(π)                  │
└─────────────────────────────────────┘

增量式平均更新:
┌─────────────────────────────────────┐
│ N ← N + 1                           │
│ V ← V + (1/N) * (G - V)             │
│ 等价于: V ← V + α(G - V)             │
│ 其中 α = 1/N                        │
└─────────────────────────────────────┘

TD更新通用模板:
┌─────────────────────────────────────┐
│ # Bellman backup
│ target = r + γ * V(s')              │
│ # TD error
│ delta = target - V(s)               │
│ # Update
│ V(s) ← V(s) + α * delta             │
└─────────────────────────────────────┘

GAE (广义优势估计):
┌─────────────────────────────────────┐
│ GAE = Σ(γλ)ⁿ δ_{t+n}  (n从0)       │
│ δ_t = r_t + γV(s_{t+1}) - V(s_t)   │
│ A(s,t) = GAE                        │
│ returns = A + V(s_t)                │
└─────────────────────────────────────┘


【超参数指南】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

学习率 α:
  - 推荐范围: 0.001 - 0.1
  - 太大: 不收敛，震荡
  - 太小: 收敛慢
  - 常见值: 0.01, 0.05, 0.1
  - 策略: 衰减 α_t = α_0 * decay_rate^t

折扣因子 γ:
  - 推荐范围: 0.9 - 0.99
  - γ = 0: 只关心即时奖励
  - γ → 1: 关心长期奖励
  - 常见值: 0.9, 0.95, 0.99
  - 物理意义: 未来奖励的重要性

探索率 ε:
  - 推荐范围: 0.01 - 0.3
  - 初始: 较高（多探索）
  - 衰减: ε_t = ε_0 * (0.995)^t
  - 最小: 0.01 （防止完全贪心）

批大小 batch_size:
  - 推荐: 32-64 (小网络)
            128-256 (大网络)

ReplayBuffer 大小:
  - 推荐: 10倍左右的episode长度
  - 范围: 1000-1000000

网络大小 hidden_units:
  - 推荐: 64-128 (小问题)
           256-512 (中等问题)
  - 3层网络通常足够


【调试技巧】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

学习不收敛:
  ✓ 检查折扣因子 γ (应该 0.9-0.99)
  ✓ 降低学习率 α
  ✓ 增加探索 (增大 ε)
  ✓ 检查奖励函数是否合理
  ✓ 增加训练轮数

收敛太慢:
  ✓ 增加学习率 α
  ✓ 检查是否有奖励稀疏问题
  ✓ 使用reward shaping
  ✓ 尝试更高效的算法 (TD vs MC)

价值估计震荡:
  ✓ 降低学习率
  ✓ 增加ReplayBuffer大小
  ✓ 使用双网络 (Double Q-Learning)

探索不充分:
  ✓ 增加探索率 ε
  ✓ 使用更柔和的策略 (Softmax)
  ✓ 更长的衰减时间表


【常见错误】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

❌ 错误: V(s) ← V(s) + α * G
✓ 正确: V(s) ← V(s) + α * (G - V(s))

❌ 错误: Q(s,a) ← r + γ * max Q(s',a')
✓ 正确: Q(s,a) ← Q(s,a) + α[r + γ*max Q - Q(s,a)]

❌ 错误: 政策评估时使用贪心策略
✓ 正确: 使用当前策略 π(a|s)

❌ 错误: TD(0)适用于长episode
✓ 正确: TD(0)适合在线学习，episode长度无关

❌ 错误: 策略梯度时直接使用Q值
✓ 正确: 应该用Q值减去基准函数作为优势


【数学速查】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

几何级数求和:
  Σ_{k=0}^∞ γ^k = 1/(1-γ)  (|γ|<1时)
  
期望的线性性:
  E[aX + bY] = aE[X] + bE[Y]
  
条件期望迭代律:
  E[X] = E[E[X|Y]]
  
log-trick (梯度):
  ∇_θ log π(a|s;θ) = ∇_θ π(a|s;θ) / π(a|s;θ)
  
Softmax导数:
  ∇ σ(x) = σ(x) * (I - σ(x)^T)


【性能指标】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

样本复杂度 (Sample Complexity):
  - 达到目标性能所需的总环境交互次数
  - 越低越好
  - MC > TD > DP (通常)

计算复杂度 (Computational Complexity):
  - 每步计算所需的浮点操作数
  - DP > 无模型方法
  
收敛速度:
  - 学习曲线的斜率
  - 通常用moving average衡量
  
方差 (Variance):
  - 相同条件下不同runs的差异
  - 低方差更稳定但可能有偏
  
偏差 (Bias):
  - 估计值与真实值的系统差异
  - 无偏方法更精确但可能高方差


【快速决策树】
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

我有完整的环境模型吗?
  是 → 使用动态规划
       ├─ 策略迭代 (更精确)
       └─ 价值迭代 (更快)
  否 → 我能采集完整episode吗?
       是 → 使用蒙特卡洛
       否 → 使用时序差分
           ├─ Q-Learning (学最优)
           ├─ Sarsa (学当前)
           └─ Expected Sarsa (平衡)

需要连续动作吗?
  是 → 策略梯度
       ├─ REINFORCE (简单，高方差)
       └─ Actor-Critic (高效)
  否 → 可以用价值方法或策略梯度都可


════════════════════════════════════════════════════════════════════════════════

💡 记住这些要点:
  1. 贝尔曼方程是核心 - 所有方法都是在求解它
  2. 自举(bootstrapping) - 用估计更新估计
  3. 探索vs利用 - ε-贪心是最简单的平衡
  4. 价值 vs 策略 - 两种思路，互有优劣
  5. 偏差vs方差 - 权衡是关键

════════════════════════════════════════════════════════════════════════════════

快速参考表打印版本:
  查看本文件中的所有表格，可以打印作为速查手册

最后更新: 2025-12-15
"""

if __name__ == '__main__':
    print(QUICK_REFERENCE)
    
    # 可选保存
    import sys
    if '--save' in sys.argv:
        output_path = '/home/ryan/2repo/my_infra/RL/QUICK_REFERENCE.txt'
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(QUICK_REFERENCE)
        print(f"\\n✓ 已保存到 {output_path}")
