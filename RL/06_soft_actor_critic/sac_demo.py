#!/usr/bin/env python3
"""
SAC Demo - GridWorldç¯å¢ƒä¸Šçš„æ¼”ç¤ºå’Œæµ‹è¯•
"""

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import numpy as np
import torch
import matplotlib.pyplot as plt
from sac_minimal import SACAgent

# å¯¼å…¥ç¯å¢ƒï¼ˆGridWorldï¼‰
try:
    from envs.gridworld import GridWorld
except ImportError:
    print("è­¦å‘Šï¼šæ— æ³•å¯¼å…¥GridWorldç¯å¢ƒ")
    GridWorld = None


def demo_sac_training():
    """
    SACåœ¨GridWorldç¯å¢ƒä¸Šçš„è®­ç»ƒæ¼”ç¤º
    """
    print("\n" + "="*80)
    print("SAC (Soft Actor-Critic) GridWorld æ¼”ç¤º".center(80))
    print("="*80 + "\n")
    
    # æ£€æŸ¥ç¯å¢ƒ
    if GridWorld is None:
        print("âŒ é”™è¯¯ï¼šGridWorldç¯å¢ƒä¸å¯ç”¨")
        print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
        return
    
    # åˆ›å»ºç¯å¢ƒ
    print("1ï¸âƒ£  åˆ›å»ºç¯å¢ƒ...")
    env = GridWorld(grid_size=4, max_steps=20)
    state_dim = 16  # 4x4 = 16ç§çŠ¶æ€
    action_dim = 4  # ä¸Šä¸‹å·¦å³
    
    print(f"   âœ“ ç¯å¢ƒå·²åˆ›å»º: {state_dim}ä¸ªçŠ¶æ€, {action_dim}ä¸ªåŠ¨ä½œ")
    print(f"   âœ“ æœ€å¤§æ­¥æ•°: {env.max_steps}\n")
    
    # åˆ›å»ºSAC Agent
    print("2ï¸âƒ£  åˆå§‹åŒ–SAC Agent...")
    agent = SACAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=128,
        learning_rate=3e-4,
        gamma=0.99,
        tau=0.005,
        alpha=0.2,
        device='cpu'
    )
    print(f"   âœ“ Agentå·²åˆå§‹åŒ–")
    print(f"   âœ“ Actorç½‘ç»œ: è¾“å…¥{state_dim} â†’ éšå±‚128 â†’ å‡å€¼/stdÃ—{action_dim}")
    print(f"   âœ“ Criticç½‘ç»œ: è¾“å…¥{state_dim+action_dim} â†’ éšå±‚128 â†’ Qå€¼\n")
    
    # è®­ç»ƒé…ç½®
    print("3ï¸âƒ£  è®­ç»ƒé…ç½®...")
    num_episodes = 100
    batch_size = 32
    update_freq = 4
    
    print(f"   âœ“ æ€»å›åˆæ•°: {num_episodes}")
    print(f"   âœ“ æ‰¹å¤§å°: {batch_size}")
    print(f"   âœ“ æ›´æ–°é¢‘ç‡: æ¯{update_freq}æ­¥æ›´æ–°ä¸€æ¬¡\n")
    
    # è®­ç»ƒå¾ªç¯
    print("4ï¸âƒ£  å¼€å§‹è®­ç»ƒ...\n")
    
    episode_rewards = []
    episode_lengths = []
    actor_losses = []
    critic_losses = []
    
    for episode in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0
        episode_length = 0
        
        # äº¤äº’ä¸€ä¸ªå›åˆ
        done = False
        while not done and episode_length < env.max_steps:
            # é€‰æ‹©åŠ¨ä½œï¼ˆä»è¿ç»­åˆ†å¸ƒé‡‡æ ·ï¼‰
            with torch.no_grad():
                state_tensor = torch.FloatTensor([state]).to(agent.device)
                action_continuous, _ = agent.actor.sample(state_tensor)
                # å°†è¿ç»­åŠ¨ä½œ[0,1]æ˜ å°„åˆ°ç¦»æ•£åŠ¨ä½œ
                action = int((action_continuous[0, 0].item() + 1) / 2 * action_dim) % action_dim
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, _ = env.step(action)
            episode_reward += reward
            episode_length += 1
            
            # å­˜å‚¨ç»éªŒ
            agent.buffer.push(state, action, reward, next_state, done)
            
            # æ›´æ–°
            if len(agent.buffer) > batch_size and episode_length % update_freq == 0:
                actor_loss, critic_loss = agent.update(batch_size)
                actor_losses.append(actor_loss)
                critic_losses.append(critic_loss)
            
            state = next_state
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        # å®šæœŸæ‰“å°è¿›åº¦
        if (episode + 1) % 20 == 0:
            avg_reward = np.mean(episode_rewards[-20:])
            avg_length = np.mean(episode_lengths[-20:])
            print(f"   å›åˆ {episode+1:3d}/{num_episodes} | "
                  f"å¥–åŠ±: {avg_reward:6.2f} | "
                  f"æ­¥æ•°: {avg_length:5.1f}")
    
    print("\n   âœ“ è®­ç»ƒå®Œæˆï¼\n")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("5ï¸âƒ£  è®­ç»ƒç»“æœç»Ÿè®¡:\n")
    print(f"   æœ€ç»ˆ100å›åˆå¹³å‡å¥–åŠ±: {np.mean(episode_rewards[-100:]):.2f}")
    print(f"   å¥–åŠ±èŒƒå›´: [{np.min(episode_rewards):.2f}, {np.max(episode_rewards):.2f}]")
    print(f"   å¹³å‡å›åˆé•¿åº¦: {np.mean(episode_lengths):.2f}")
    print(f"   æ€»æ›´æ–°æ­¥æ•°: {len(actor_losses)}\n")
    
    # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
    print("6ï¸âƒ£  ç»˜åˆ¶å­¦ä¹ æ›²çº¿...")
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # å¥–åŠ±æ›²çº¿
    ax = axes[0, 0]
    ax.plot(episode_rewards, label='Episode Reward')
    ax.plot(np.convolve(episode_rewards, np.ones(10)/10, mode='valid'), 
            label='MA(10)', linewidth=2)
    ax.set_xlabel('Episode')
    ax.set_ylabel('Reward')
    ax.set_title('SAC Training: Episode Rewards')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # å›åˆé•¿åº¦
    ax = axes[0, 1]
    ax.plot(episode_lengths, alpha=0.5, label='Episode Length')
    ax.plot(np.convolve(episode_lengths, np.ones(10)/10, mode='valid'),
            label='MA(10)', linewidth=2)
    ax.set_xlabel('Episode')
    ax.set_ylabel('Steps')
    ax.set_title('SAC Training: Episode Length')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # ActoræŸå¤±
    ax = axes[1, 0]
    if actor_losses:
        ax.plot(actor_losses, alpha=0.5, label='Actor Loss')
        ax.plot(np.convolve(actor_losses, np.ones(10)/10, mode='valid'),
                label='MA(10)', linewidth=2)
    ax.set_xlabel('Update Step')
    ax.set_ylabel('Loss')
    ax.set_title('Actor Loss')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # CriticæŸå¤±
    ax = axes[1, 1]
    if critic_losses:
        ax.plot(critic_losses, alpha=0.5, label='Critic Loss')
        ax.plot(np.convolve(critic_losses, np.ones(10)/10, mode='valid'),
                label='MA(10)', linewidth=2)
    ax.set_xlabel('Update Step')
    ax.set_ylabel('Loss')
    ax.set_title('Critic Loss')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('sac_training_curves.png', dpi=100)
    print("   âœ“ æ›²çº¿å·²ä¿å­˜: sac_training_curves.png\n")
    
    # æµ‹è¯•é˜¶æ®µ
    print("7ï¸âƒ£  è¯„ä¼°æ¨¡å¼ï¼ˆè´ªå¿ƒç­–ç•¥ï¼‰:\n")
    test_episodes = 10
    test_rewards = []
    
    for episode in range(test_episodes):
        state, _ = env.reset()
        episode_reward = 0
        done = False
        steps = 0
        
        while not done and steps < env.max_steps:
            with torch.no_grad():
                state_tensor = torch.FloatTensor([state]).to(agent.device)
                # ä½¿ç”¨å‡å€¼ï¼ˆè´ªå¿ƒï¼‰
                mean, _ = agent.actor.forward(state_tensor)
                action = int((mean[0, 0].item() + 1) / 2 * action_dim) % action_dim
            
            next_state, reward, done, _ = env.step(action)
            episode_reward += reward
            state = next_state
            steps += 1
        
        test_rewards.append(episode_reward)
        print(f"   æµ‹è¯•å›åˆ {episode+1:2d}: å¥–åŠ± = {episode_reward:6.2f}")
    
    print(f"\n   å¹³å‡æµ‹è¯•å¥–åŠ±: {np.mean(test_rewards):.2f}")
    print(f"   æµ‹è¯•å¥–åŠ±èŒƒå›´: [{np.min(test_rewards):.2f}, {np.max(test_rewards):.2f}]\n")
    
    print("="*80)
    print("âœ… SACæ¼”ç¤ºå®Œæˆï¼".center(80))
    print("="*80 + "\n")
    
    return agent, env, episode_rewards


def explain_sac_key_concepts():
    """
    è§£é‡ŠSACçš„å…³é”®æ¦‚å¿µ
    """
    print("\n" + "="*80)
    print("SAC å…³é”®æ¦‚å¿µè®²è§£".center(80))
    print("="*80 + "\n")
    
    concepts = {
        "1. æœ€å¤§ç†µç›®æ ‡": """
ç›®æ ‡å‡½æ•°ï¼šJ(Ï€) = E[r_t + Î± H(Ï€)]
  - ä¸ä»…æœ€å¤§åŒ–å¥–åŠ± r_t
  - è¿˜æœ€å¤§åŒ–ç­–ç•¥ç†µ H(Ï€) = -E[log Ï€(a|s)]
  - Î±æƒè¡¡ä¸¤è€…çš„é‡è¦æ€§
  
å¥½å¤„ï¼š
  - é¼“åŠ±æ¢ç´¢ï¼šä¸ä¼šè¿‡æ—©æ”¶æ•›
  - å­¦ä¹ å¤šæ¨¡æ€ç­–ç•¥ï¼šå¤šæ¡æœ€ä¼˜è·¯å¾„
  - æé«˜é²æ£’æ€§ï¼šå¯¹ç¯å¢ƒå˜åŒ–å®¹é”™
""",
        
        "2. é‡å‚æ•°åŒ–æŠ€å·§": """
é‡‡æ ·æµç¨‹ï¼š
  z ~ N(Î¼(s), ÏƒÂ²(s))     é«˜æ–¯é‡‡æ ·
  a = tanh(z)            å‹ç¼©åˆ°[-1,1]
  
ä¸ºä»€ä¹ˆï¼Ÿ
  - ç›´æ¥é‡‡æ · a~Ï€ ä¸å¯å¾®
  - é‡å‚æ•°åŒ–ä½¿æ¢¯åº¦èƒ½æµå‘å‚æ•°Î¼å’ŒÏƒ
  - å…³é”®ï¼šlog Ï€(a|s) = log p(z) - log|1-aÂ²|
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                æ¢¯åº¦é¡¹         é›…å¯æ¯”ä¿®æ­£ï¼ˆé‡è¦ï¼ï¼‰
""",
        
        "3. åŒQç½‘ç»œ": """
è®¾è®¡ï¼šä¸¤ä¸ªç‹¬ç«‹çš„Qç½‘ç»œ Q_1 å’Œ Q_2
      ç›®æ ‡å€¼ = min(Q_1_target, Q_2_target)
  
ä¸ºä»€ä¹ˆï¼Ÿ
  - å•ä¸ªQå®¹æ˜“ç³»ç»Ÿæ€§é«˜ä¼°åŠ¨ä½œä»·å€¼
  - ä¸¤ä¸ªç‹¬ç«‹ç½‘ç»œçš„æœ€å°å€¼æ›´ä¿å®ˆ
  - å¤§å¹…æé«˜å­¦ä¹ ç¨³å®šæ€§
  
æƒè¡¡ï¼š
  - è®¡ç®—é‡â†‘ï¼ˆä¸¤ä¸ªç½‘ç»œï¼‰
  - ç¨³å®šæ€§â†‘â†‘ï¼ˆæœ€é‡è¦ï¼‰
""",
        
        "4. è‡ªé€‚åº”æ¸©åº¦": """
å‚æ•°ï¼šÎ±ï¼ˆç†µç³»æ•°ï¼‰ï¼Œé€šè¿‡ log_Î± å­¦ä¹ 

è‡ªåŠ¨è°ƒèŠ‚ï¼š
  ç›®æ ‡ç†µ H_target = -action_dim
  
  å¦‚æœ H(Ï€) < H_targetï¼š
    â†’ loss > 0, log_Î±â†‘, Î±â†‘
    â†’ æ›´å¤šå¥–åŠ±ç»™ç†µ â†’ é¼“åŠ±æ¢ç´¢
  
  å¦‚æœ H(Ï€) > H_targetï¼š
    â†’ loss < 0, log_Î±â†“, Î±â†“
    â†’ å‡å°‘å¥–åŠ±ç»™ç†µ â†’ ä¸“æ³¨é«˜å›æŠ¥
  
å¥½å¤„ï¼šä¸éœ€è¦æ‰‹åŠ¨è°ƒæ•´æ¢ç´¢ç¨‹åº¦ï¼
""",
        
        "5. ä¸‰ä¸ªç½‘ç»œæ›´æ–°": """
æ¯æ­¥æ›´æ–°ä¸‰ä¸ªç›®æ ‡ï¼š

1ï¸âƒ£  è¯„è®ºå®¶(Critic)æ›´æ–°ï¼š
   æœ€å°åŒ–: L_Q = (Q(s,a) - y)Â²
   å…¶ä¸­:   y = r + Î³ V(s') = r + Î³(Q(s',a') - Î± log Ï€(a'|s'))

2ï¸âƒ£  æ¼”å‘˜(Actor)æ›´æ–°ï¼š
   æœ€å°åŒ–: L_Ï€ = Î± log Ï€(a|s) - Q(s,a)
   æ•ˆæœ:   å¢åŠ é«˜Qå€¼åŠ¨ä½œæ¦‚ç‡ + å¢åŠ ç†µ

3ï¸âƒ£  æ¸©åº¦(Alpha)æ›´æ–°ï¼š
   æœ€å°åŒ–: L_Î± = -Î±(log Ï€(a|s) + H_target)
   æ•ˆæœ:   è‡ªåŠ¨è°ƒèŠ‚æ¢ç´¢ç¨‹åº¦
"""
    }
    
    for title, explanation in concepts.items():
        print(f"{title}")
        print("-" * 80)
        print(explanation)
        print()
    
    print("="*80 + "\n")


def main():
    """ä¸»ç¨‹åº"""
    print("\n" + "ğŸš€ "*40)
    print("\næ¬¢è¿æ¥åˆ°SAC (Soft Actor-Critic) å­¦ä¹ æ•™ç¨‹\n")
    print("ğŸš€ "*40 + "\n")
    
    while True:
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("  1. æŸ¥çœ‹SACå…³é”®æ¦‚å¿µè®²è§£")
        print("  2. è¿è¡ŒSACåœ¨GridWorldä¸Šçš„æ¼”ç¤º")
        print("  3. æŸ¥çœ‹SACç†è®ºæ–‡æ¡£")
        print("  4. æŸ¥çœ‹SACå®Œæ•´æŒ‡å—")
        print("  5. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()
        
        if choice == '1':
            explain_sac_key_concepts()
        
        elif choice == '2':
            try:
                demo_sac_training()
            except Exception as e:
                print(f"\nâŒ æ¼”ç¤ºå‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
        
        elif choice == '3':
            print("\nğŸ“š SACç†è®ºæ–‡æ¡£ä½ç½®: SAC_EXPLANATION.md")
            print("   åŒ…å«å†…å®¹:")
            print("   - æ ¸å¿ƒæ€æƒ³å’Œä¸ºä»€ä¹ˆè¦æœ€å¤§åŒ–ç†µ")
            print("   - æ•°å­¦åŸç†ï¼ˆç›®æ ‡å‡½æ•°ã€è´å°”æ›¼æ–¹ç¨‹ï¼‰")
            print("   - é‡å‚æ•°åŒ–æŠ€å·§è¯¦è§£")
            print("   - ç½‘ç»œæ¶æ„å’Œç®—æ³•æµç¨‹")
            print("   - æ•°å€¼ç¨³å®šæ€§æŠ€å·§")
            print("   - è¶…å‚æ•°è°ƒæ•´æŒ‡å—")
            print("   - SAC vså…¶ä»–ç®—æ³•å¯¹æ¯”")
            print("\n   åœ¨é¡¹ç›®æ ¹ç›®å½•æ‰“å¼€: cat 06_soft_actor_critic/SAC_EXPLANATION.md")
        
        elif choice == '4':
            print("\nğŸ“– SACå®Œæ•´æŒ‡å—ä½ç½®: sac_guide.py")
            print("   åŒ…å«å†…å®¹:")
            print("   - 10éƒ¨åˆ†çš„è¯¦ç»†è®²è§£")
            print("   - æ ¸å¿ƒæ€æƒ³ â†’ æ•°å­¦åŸç† â†’ å®ç°ç»†èŠ‚")
            print("   - ä¼˜ç¼ºç‚¹åˆ†æ")
            print("   - ä¸å…¶ä»–ç®—æ³•å¯¹æ¯”")
            print("   - å¸¸è§é—®é¢˜è§£ç­”")
            print("   - é«˜çº§è¯é¢˜ï¼ˆç¦»çº¿RLã€åˆ†å¸ƒå¼ç­‰ï¼‰")
            print("\n   è¿è¡ŒæŸ¥çœ‹: python sac_guide.py")
        
        elif choice == '5':
            print("\nğŸ‘‹ è°¢è°¢ä½¿ç”¨ï¼ç»§ç»­å­¦ä¹ SACå§ï¼\n")
            break
        
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")


if __name__ == '__main__':
    # å¦‚æœæœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œç›´æ¥è¿è¡Œå¯¹åº”åŠŸèƒ½
    if len(sys.argv) > 1:
        if sys.argv[1] == '--demo':
            demo_sac_training()
        elif sys.argv[1] == '--concepts':
            explain_sac_key_concepts()
        elif sys.argv[1] == '--guide':
            import sac_guide
        elif sys.argv[1] == '--help':
            print("ç”¨æ³•: python sac_demo.py [é€‰é¡¹]")
            print("é€‰é¡¹:")
            print("  --demo: è¿è¡Œæ¼”ç¤º")
            print("  --concepts: æ˜¾ç¤ºå…³é”®æ¦‚å¿µ")
            print("  --guide: æ˜¾ç¤ºå®Œæ•´æŒ‡å—")
            print("  --help: æ˜¾ç¤ºæ­¤å¸®åŠ©")
            print("\nä¸å¸¦å‚æ•°æ—¶è¿›å…¥äº¤äº’èœå•")
    else:
        main()
